---
title: "Problem-Set-1 for POLI 271"
format: pdf
editor: visual
---

# 1. Univariate displays & sampling distributions

## *Problem a*
```{r, fig.width=7, fig.height=5}
s_values = c(10, 100, 1000)
n_values = c(10, 100, 1000)

par(mfrow = c(3, 3))
for (s in s_values) {
  for (n in n_values) {
    sample_means = replicate(s, mean(sample(1:20, n, replace = TRUE)))

    hist(sample_means,
         breaks = 20,
         col = "grey",
         border = "black",
         main = paste("s =", s, ", n =", n),
         xlab = "",
         xlim = c(1, 20))
    
    abline(v = mean(sample_means), col = "red", lwd = 0.5)
  }
}


```
## *Problem b*
The histograms shows that when increasing the sample size reduces variability in sample means. I set it to 1:20 so it's getting closer to 10. And increasing the number of samples makes it look like normal distribution. The key assumption here is what CLT describes: the distribution of a normalized version of the sample mean converges to a standard normal distribution. 

# 2. Monte Carlo integration

```{r}
func = function(x){exp(-x)*sin(x)}
result = integrate(func, lower = 2, upper = 5)
print(result)
```
# 3. Systematic and stochastic components

## *Problem a*

Systematic Component: $y_i=1+0.5x_{i1}-2.2x_{i2}+x_{i3}$
Stochastic Component: ${\epsilon_i}\sim{N(\mu=0,\sigma^2=1.5)}$

## *Problem b*
**Part I.** The dimensions of $\mathbf{X}$ is denoted as $n$ is $2$. 
```{r}
data = read.csv("xmat.csv")
print(dim(data))
head(data, 10)
```
**Part II.**

```{r}
set.seed(10825) 
# Why not 42 and 3407
coefficient_0 = 1
coefficient_1 = 0.5
coefficient_2 = -2.2
coefficient_3 = 1
x_1 = data$X1
x_2 = data$X2
x_3 = data$X3
e = rnorm(n = nrow(data), mean = 0, sd = sqrt(1.5))
y = coefficient_0 + coefficient_1*x_1 + coefficient_2*x_2 + coefficient_3*x_3 + e

linear = lm(y ~ x_1 + x_2 + x_3)
summary((linear))
# Beautiful p-value
```
# 4. OLS in matrix form

```{r}
library(haven)
data_2 = read_dta("coxappend.dta")
attributes(data_2)
```
```{r}
head(data_2, 10)
```
## *Problem a*

```{r}
ols_regression = function(y, X) {
  X = cbind(1, X)
  
  # beta_hat = (X'X)^(-1) X'y
  beta_hat = solve(t(X) %*% X) %*% t(X) %*% y
  
  # residuals = y - X * beta_hat
  residuals = y - X %*% beta_hat
  
  # sigma^2 = RSS / (n - p)
  n = nrow(X)
  p = ncol(X)
  sigma2 = sum(residuals^2) / (n - p)
  
  # SE(beta) = sqrt(diag(sigma^2 * (X'X)^(-1)))
  se_beta = sqrt(diag(sigma2 * solve(t(X) %*% X)))
  
  return(list(
    coefficients = beta_hat,
    standard_errors = se_beta,
    residuals = residuals
  ))
}

y = data_2$enps
X = data.frame(
  eneth = data_2$eneth,
  log_ml = log(data_2$ml),
  interaction = data_2$eneth * log(data_2$ml)
)

results = ols_regression(y, as.matrix(X))

print("OLS Results:")
coefficients_table = data.frame(
  Coefficient = results$coefficients,
  Std_Error = results$standard_errors
)
rownames(coefficients_table) = c("Intercept", "eneth", "log_ml", "interaction")
print(coefficients_table)

```

## *Problem b*

```{r}
qq_plot = function(residuals) {
  qqnorm(residuals, main = "Q-Q Plot of Residuals")
  qqline(residuals, col = "purple", lwd = 2)
}

residuals = results$residuals
qq_plot(residuals)

```
- The Q-Q plot shows that the residuals mostly align with the reference line. 
- The deviations at the tails ndicates outliers in the residual distribution. 
- Overall, the residuals are close to a normal distribution.

## *Problem c*

```{r}
lm_model = lm(enps ~ eneth * log(ml), data = data_2)
summary(lm_model)
```
## *Problem d*

```{r}
#| fig-width: 6
#| fig-height: 6
par(mfrow = c(2, 2))
plot(lm_model)
```
The OLS model gives us some useful insights, but it’s not a perfect fit for the data. The patterns in the plots suggest the relationships might not be completely linear, the spread of the errors isn’t consistent, and a few points have a big influence on the results. 

## **Appendix**

I certify that we did not use any LLM or generative AI tool in this assignment. 